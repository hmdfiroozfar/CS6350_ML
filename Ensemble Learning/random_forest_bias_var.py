# -*- coding: utf-8 -*-
"""Random Forest Bias Var.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b47Btg3or7WbeXcTo_8IK9zsY_h4yS_p
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
from random import randint
from random import sample
import copy
from Adaboost import *

COLUMN_NAMES = [
    'age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan',
    'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y'
]

COLUMN_TYPES = [
    'numeric', 'categorical', 'categorical', 'categorical', 'binary', 'numeric',
    'binary', 'binary', 'categorical', 'numeric', 'categorical', 'numeric',
    'numeric', 'numeric', 'numeric', 'categorical', 'binary'
]

COLUMN_DICT = dict(zip(COLUMN_NAMES, COLUMN_TYPES))

def process_data(train_path, test_path, column_dict, column_names):
    train_data = pd.read_csv(train_path, names=column_names)
    test_data = pd.read_csv(test_path, names=column_names)

    median_dict = {}
    processed_train = pd.DataFrame()
    processed_test = pd.DataFrame()
    for column in column_names:
        if column_dict[column] == 'numeric':
            median_val = train_data[column].median()
            median_dict[column] = median_val
            processed_train[column + '>' + str(median_val)] = np.where(train_data[column] > median_val, 'yes', 'no')
            processed_test[column + '>' + str(median_val)] = np.where(test_data[column] > median_val, 'yes', 'no')
        else:
            processed_train[column] = train_data[column]
            processed_test[column] = test_data[column]

    processed_train_values = [list(processed_train.loc[i]) for i in range(len(processed_train))]
    processed_test_values = [list(processed_test.loc[i]) for i in range(len(processed_test))]

    return processed_train_values, processed_test_values

def convert_labels(labels):
    return [1 if label == 'yes' else -1 for label in labels]


processed_train, processed_test = process_data('Data/train.csv', 'Data/test.csv', COLUMN_DICT, COLUMN_NAMES)

training_data = [sample[:-1] for sample in processed_train]
training_labels = convert_labels([sample[-1] for sample in processed_train])

testing_data = [sample[:-1] for sample in processed_test]
testing_labels = convert_labels([sample[-1] for sample in processed_test])


def sample_data(X, Y, n_samples=None):

    if n_samples is None:
        n_samples = len(X)

    indices = [random.randint(0, len(X)-1) for _ in range(n_samples)]
    sampled_X = [X[i] for i in indices]
    sampled_Y = [Y[i] for i in indices]

    return sampled_X, sampled_Y

num_samples = 1000
num_bags = 100
num_trees = 50
model_history = []

for _ in range(num_bags):
    forest = []
    for _ in range(num_trees):
        X_sample, Y_sample = sample_data(training_data, training_labels, n_samples=num_samples)
        model = DecisionTree(X_sample, Y_sample, attributes=[i for i in range(len(training_data[0]))], depth=-1, feature_sample_size=2)
        forest.append(copy.copy(model))
    model_history.append(forest)



def single_bias_var(x, y, forest):
    """
    Calculate bias and variance for a single observation.
    """
    predictions = np.array([model.predict(x) for model in forest])
    bias = (y - predictions.mean())**2
    variance = predictions.var()

    return bias, variance

forest_of_first_models = [bag[0] for bag in model_history]

bias_single, variance_single = np.mean([single_bias_var(testing_data[i], testing_labels[i], forest_of_first_models) for i in range(len(testing_data))], axis=0)

general_squared_error_single = bias_single + variance_single
print(f'For single models: Bias: {bias_single}, Variance: {variance_single}, General Squared Error: {general_squared_error_single}')



def bagging_prediction(x, bag):

    predictions = sum(model.predict(x) for model in bag)

    return 1 if predictions >= 0 else -1

def bagging_bias_var(x, y, list_of_bags):

    predictions = np.array([bagging_prediction(x, bag) for bag in list_of_bags])
    bias = (y - predictions.mean())**2
    variance = predictions.var()

    return bias, variance

bias_bagging, variance_bagging = np.mean([bagging_bias_var(testing_data[i], testing_labels[i], model_history) for i in range(len(testing_data))], axis=0)

print(f'For bagging: Bias: {bias_bagging}, Variance: {variance_bagging}, General Squared Error: {bias_bagging + variance_bagging}')