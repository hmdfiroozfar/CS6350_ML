# -*- coding: utf-8 -*-
"""Adaboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ETtJqLDE6j1qs-JsKrE6URuOJoqRJ8E
"""

from numpy import log2

class Metrics:
    @staticmethod
    def _initialize_weights(labels, weights=None):
        if weights is None:
            weights = [1] * len(labels)
        return weights

    @staticmethod
    def Gini(labels, weights=None):
        weights = Metrics._initialize_weights(labels, weights)
        weight_distribution, total_weight = Metrics._get_weight_distribution(labels, weights)
        gini_sum = sum([(weight_distribution[label]/total_weight)**2 for label in weight_distribution])
        return 1 - gini_sum

    @staticmethod
    def majority(labels, weights=None):
        weights = Metrics._initialize_weights(labels, weights)
        weight_distribution, max_weight, _ = Metrics._get_max_weight_distribution(labels, weights)
        return 1 - max_weight/sum(weights)

    @staticmethod
    def entropy(labels, weights=None):
        weights = Metrics._initialize_weights(labels, weights)
        weight_distribution, total_weight = Metrics._get_weight_distribution(labels, weights)
        entropy_sum = sum([(weight_distribution[label]/total_weight) * log2(total_weight/weight_distribution[label]) for label in weight_distribution])
        return entropy_sum

    @staticmethod
    def Majority(labels, weights=None, ignore_value=None):
        weights = Metrics._initialize_weights(labels, weights)
        label_weights = {}
        for i in range(len(labels)):
            if labels[i] not in label_weights:
                label_weights[labels[i]] = 0
            if ignore_value is None or labels[i] != ignore_value:
                label_weights[labels[i]] += weights[i]
        majority_label, max_weight = max(label_weights.items(), key=lambda item: item[1])
        return majority_label, len(label_weights)

    @staticmethod
    def _get_weight_distribution(labels, weights):
        label_weights = {}
        total_weight = 0
        for i in range(len(labels)):
            label_weights.setdefault(labels[i], 0)
            label_weights[labels[i]] += weights[i]
            total_weight += weights[i]
        return label_weights, total_weight

    @staticmethod
    def _get_max_weight_distribution(labels, weights):
        label_weights, total_weight = Metrics._get_weight_distribution(labels, weights)
        majority_label, max_weight = max(label_weights.items(), key=lambda item: item[1])
        return label_weights, max_weight, majority_label

class DecisionTree:
    def __init__(self, training_data, labels, attributes, depth=-1, weights=None, entropy_function=Metrics.entropy, feature_sample_size=None):
        self.feature_sample_size = feature_sample_size
        self.entropy_function = entropy_function
        self.leaf = False
        self.label, num_values = Metrics.Majority(labels, weights)
        if not attributes or num_values == 1 or depth == 0:
            self.leaf = True
            return
        self.split_attribute, attribute_values = self._best_attribute(training_data, labels, attributes, weights)
        split_data, split_labels, split_weights = self._split_by_attribute(training_data, labels, self.split_attribute, weights)
        self.sub_trees = {}
        attributes.remove(self.split_attribute)
        for value in split_data:
            self.sub_trees[value] = DecisionTree(split_data[value], split_labels[value], attributes, depth - 1, split_weights[value], entropy_function)
        attributes.append(self.split_attribute)

    def predict(self, instance):
        if self.leaf:
            return self.label
        if instance[self.split_attribute] in self.sub_trees:
            return self.sub_trees[instance[self.split_attribute]].predict(instance)
        return self.label

    def _best_attribute(self, training_data, labels, attributes, weights):
        sampled_attributes = attributes if not self.feature_sample_size else sample(attributes, min(len(attributes), self.feature_sample_size))
        overall_entropy = Metrics.entropy(labels, weights)
        max_gain, best_attribute, best_attribute_values = -1, None, None
        for attribute in sampled_attributes:
            entropy, attribute_values = self._entropy_for_attribute(training_data, labels, attribute, weights)
            gain = overall_entropy - entropy
            if gain > max_gain:
                max_gain, best_attribute, best_attribute_values = gain, attribute, attribute_values
        return best_attribute, best_attribute_values

    def _split_by_attribute(self, training_data, labels, attribute, weights=None):
        num_instances = len(labels)
        if weights is None:
            weights = [1] * num_instances
        attribute_weights, attribute_data, attribute_labels = {}, {}, {}
        for i in range(num_instances):
            attribute_value = training_data[i][attribute]
            attribute_weights.setdefault(attribute_value, [])
            attribute_data.setdefault(attribute_value, [])
            attribute_labels.setdefault(attribute_value, [])
            attribute_weights[attribute_value].append(weights[i])
            attribute_data[attribute_value].append(training_data[i])
            attribute_labels[attribute_value].append(labels[i])
        return attribute_data, attribute_labels, attribute_weights

    def _entropy_for_attribute(self, data, labels, attribute, weights=None):
        num_labels = len(labels)
        if weights is None:
            weights = [1] * num_labels
        attribute_labels, attribute_weights, total_weight = {}, {}, sum(weights)
        for i in range(num_labels):
            attribute_value = data[i][attribute]
            attribute_weights.setdefault(attribute_value, [])
            attribute_labels.setdefault(attribute_value, [])
            attribute_weights[attribute_value].append(weights[i])
            attribute_labels[attribute_value].append(labels[i])
        entropy = sum([sum(attribute_weights[value]) * self.entropy_function(attribute_labels[value], attribute_weights[value]) / total_weight for value in attribute_weights])
        return entropy, list(attribute_weights.keys())

class AdaBoost:

    def __init__(self, training_data, labels, num_iterations, tree_depth):
        self.decision_trees = []
        self.coefficients = []
        self.weights = []
        self.errors = []

        attributes = [i for i in range(len(training_data[0]))]
        self.train(training_data, labels, attributes, num_iterations, tree_depth)

    def train(self, training_data, labels, attributes, num_iterations, tree_depth):
        current_tree = None

        for _ in range(num_iterations):
            self.update_weights(training_data, labels, current_tree)
            current_tree = DecisionTree(training_data, labels, attributes=attributes, depth=tree_depth, weights=self.weights)
            self.decision_trees.append(current_tree)

    def update_weights(self, training_data, labels, tree):
        num_samples = len(labels)

        if tree is None:
            self.weights = [1/num_samples] * num_samples
            self.coefficients.append(1)
            return

        error, predictions = self.compute_error(training_data, labels, tree)
        self.errors.append(error)

        alpha = log((1-error)/error) / 2
        self.coefficients.append(alpha)

        for i in range(num_samples):
            if predictions[i]:
                self.weights[i] *= (sqrt(error/(1-error)))
            else:
                self.weights[i] *= (sqrt((1-error)/error))

        normalization_factor = sum(self.weights)
        for i in range(num_samples):
            self.weights[i] /= normalization_factor

    def compute_error(self, training_data, labels, tree):
        num_samples = len(training_data)
        predictions = [True] * num_samples
        error = 0

        for i in range(num_samples):
            predicted_label = tree.predict(training_data[i])
            if predicted_label != labels[i]:
                error += self.weights[i]
                predictions[i] = False

        return error, predictions

    def predict(self, instance, num_trees=None):
        if num_trees is None:
            num_trees = len(self.decision_trees)

        prediction_sum = 0
        for i in range(num_trees):
            prediction_sum += self.coefficients[i] * self.decision_trees[i].predict(instance)

        return 1 if prediction_sum >= 0 else -1

    def compute_overall_error(self, data, labels, num_trees):
        num_samples = len(labels)
        errors = [0] * num_trees

        for i in range(num_samples):
            prediction_sum = 0

            for j in range(num_trees):
                label = self.decision_trees[j].predict(data[i])
                prediction_sum += self.coefficients[j] * label

                if (prediction_sum >= 0 and labels[i] == -1) or (prediction_sum < 0 and labels[i] == 1):
                    errors[j] += 1/num_samples

        return errors