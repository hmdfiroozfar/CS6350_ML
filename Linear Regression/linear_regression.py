# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hgGXpXsFFihALm5mvNRnOFrP4aD_7i0y
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from random import sample

def load_data(train_path, test_path):
    training_data = np.genfromtxt(train_path, delimiter=',')
    test_data = np.genfromtxt(test_path, delimiter=',')

    train_features = np.c_[training_data[:, :-1], np.ones(training_data.shape[0])]
    train_labels = training_data[:, -1]

    test_features = np.c_[test_data[:, :-1], np.ones(test_data.shape[0])]
    test_labels = test_data[:, -1]

    return train_features, train_labels, test_features, test_labels

def compute_cost(weights, features, labels):
    return 0.5 * (features @ weights - labels).T @ (features @ weights - labels)

def compute_gradient(weights, features, labels):
    return features.T @ (features @ weights - labels)

def gradient_descent(learning_rate, features, labels):
    weights = np.zeros(8)
    cost_history = [compute_cost(weights, features, labels)]
    counter = 0
    while True:
        counter += 1
        new_weights = weights - learning_rate * compute_gradient(weights, features, labels)
        cost_history.append(compute_cost(new_weights, features, labels))
        if np.linalg.norm(weights - new_weights) < 1e-6 or counter >1e10:
            break
        weights = new_weights


    return new_weights, cost_history, learning_rate

def SGD(learning_rate, X, y):
    weights = np.zeros(8)
    cost_history = [compute_cost(X, y, weights)]
    counter = 0
    while True:
        counter += 1
        index = sample([x for x in range(len(X))], 1)[0]
        new_weights = weights - learning_rate * compute_gradient(weights, X[index].reshape(1,-1), np.array(y[index]))
        cost_history.append(compute_cost(new_weights, X, y))
        if np.linalg.norm(weights - new_weights)<1e-6 or counter > 1e5:
            weights = new_weights
            break
        else:
            weights = new_weights

    return new_weights, cost_history, learning_rate

def plot_loss(history, title):
    plt.plot(history, color='blue', label='Training loss')
    plt.title(title, color='black')
    plt.legend()
    plt.show()

def optimal_weights(features, labels):
    return np.linalg.inv(features.T @ features) @ features.T @ labels

def main(gd_lr, sgd_lr):

    train_features, train_labels, test_features, test_labels = load_data('Data/train.csv', 'Data/test.csv')

    gd_weights, gd_costs, gd_learning_rate = gradient_descent(gd_lr, train_features, train_labels)
    print(f'Final weights using GD: {gd_weights}')
    print(f"Final GD learning rate: {gd_learning_rate}")
    plot_loss(gd_costs, "Training loss vs #iterations (GD)")
    print(f'Final test cost using GD: {compute_cost(gd_weights, test_features, test_labels)}')

    sgd_weights, sgd_costs, sgd_learing_rate = SGD(sgd_lr, train_features, train_labels)
    print(f'Final weights using SGD: {sgd_weights}')
    print(f"Final SGD learning rate: {sgd_learing_rate}")
    plot_loss(sgd_costs, "Training loss vs #iterations (SGD)")
    print(f'Final test cost using SGD: {compute_cost(sgd_weights, test_features, test_labels)}')

    optimal_w = optimal_weights(train_features, train_labels)
    print(f'Optimal weight vector: {optimal_w}')
    print(f'Final test cost with optimal weights: {compute_cost(optimal_w, test_features, test_labels)}')

gd_lr = 0.015
sgd_lr = 0.05
if __name__ == "__main__":
  main(gd_lr, sgd_lr)